
    # IMPORTING LIBRARIES 

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from pandas import ExcelWriter
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, recall_score, auc, roc_curve, precision_score, f1_score, roc_auc_score, precision_recall_curve
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

################################################################################
    # LOADING DATA 
    
train_df = pd.read_csv('train.csv', encoding = "ISO-8859-1") # ok que es eso?
test_df = pd.read_csv('test.csv')
addresses = pd.read_csv('addresses.csv')
latlons = pd.read_csv('latlons.csv')

#train_df.shape



################################################################################
    # CLEANING DATA 
    
train_df.info()

#Droping Data that not provide information. For example variables with payment information, too many missing/null values, too many categories.

train_df = train_df.drop(['ticket_id','country','payment_date','payment_status','collection_status',
                          'grafitti_status','compliance_detail','violation_zip_code','non_us_str_code',
                          'payment_amount','late_fee','balance_due','state'],axis=1)
test_df = test_df.drop(['ticket_id','country','grafitti_status','violation_zip_code','non_us_str_code',
                        'late_fee','state'],axis=1)
#train_df.head()
  
  
#Dropping variables with too many unique values.

##First we visualice number of unique values


######### PLOT ojo aca!! para ver mas tarde
unique_lens = []
for var in train_df.columns:
  unique_lens.append(len(train_df[var].unique()))

plt.figure(figsize=(16,8))
plt.bar(train_df.columns,unique_lens,alpha=0.7)
plt.xticks(rotation=90,size=15)
plt.yticks(size=15)
plt.show()
########

######## Data Table 
#them explore categories with smaller number of variables
#Exploring the unique values for categorical variables that have relatively lower categories
print ("Number of unique values in violation_code:",len(train_df['violation_code'].unique()))
print ("Number of unique values in violation_description:",len(train_df['violation_description'].unique()))
print ("Number of unique values in violation_street_name:",len(train_df['violation_street_name'].unique()))
print ("Number of unique values in disposition:",len(train_df['disposition'].unique()))
print ("Number of unique values in zip_code:",len(train_df['zip_code'].unique()))
print ("Number of unique values in inspector_name:",len(train_df['inspector_name'].unique()))
########


# drop variables

train_df = train_df.drop(['violator_name','violation_street_number','mailing_address_str_number',
                          'mailing_address_str_name','violation_street_name',
                          'violation_code','violation_description','zip_code','inspector_name'],axis=1)
test_df = test_df.drop(['violator_name','violation_street_number','mailing_address_str_number',
                          'mailing_address_str_name','violation_street_name',
                          'violation_code','violation_description','zip_code','inspector_name'],axis=1)
train_df.info()

# to change all city entries to upper case 
train_df['city'].value_counts()

# Explore values in 'city' 
train_df['city'].value_counts()

#Filtering tickets only for  Detroit, because is the main data and then dropping the 'city' variable 

train_df = train_df[train_df["city"]=="DETROIT"]
train_df = train_df.drop(["city"],axis=1)
test_df = test_df.drop(["city"],axis=1)









# Analizing 'compliance' variable: The Target variable
#'compliance' [target variable for prediction] could contain 3 values:
        #Null = Not responsible
        #0 = Responsible, non-compliant
        #1 = Responsible, compliant

######## PLOT ploting the variable
plt.figure(figsize=(12,8))
sns.countplot(data=train_df,x='compliance',alpha=0.8)
plt.xticks(size=15)
plt.yticks(size=15)
plt.xlabel('Compliance',size=15)
plt.ylabel('Count',size=15)
plt.show()
########

# Exploring how much null values the target variable has
train_df['compliance'].isnull().sum()

#Removing all 'not found responsible' compliance entries
train_df = train_df[(train_df["compliance"]==1.0) | (train_df["compliance"]==0.0)]








# Exploring Categorical variables

######## ploting 
plt.figure(figsize=(12,8))
sns.countplot(data=train_df,x='agency_name',hue='compliance',alpha=0.8)
plt.xlabel('Compliance',size=15)
plt.ylabel('Count',size=15)
plt.xticks(rotation=45,size=15)
plt.yticks(size=15)
plt.show()
########

######## displaying data table
agency_name_group = train_df[['agency_name','compliance','fine_amount']].groupby(['agency_name','compliance']).count()
agency_name_group.columns = ['count']
agency_name_group['percent'] = agency_name_group.groupby(level=0).apply(lambda x: 100*(x/x.sum()))
agency_name_group
########



# Exploring numerical variables (ploting bellow) 

######## ploting for 'fine_amount'

plt.figure(figsize=(18,8))
plt.subplot(1,2,1)
sns.boxplot(data=train_df,y='fine_amount',x='compliance')
plt.title('Fine amount vs compliance')
plt.subplot(1,2,2)
sns.boxplot(data=train_df[train_df['fine_amount']<1000],y='fine_amount',x='compliance')
plt.title('Fine amount vs compliance (zoomed in)')

######## ploting for 'admin_fee'
sns.boxplot(data=train_df,y='admin_fee',x='compliance')

plt.hist(train_df['admin_fee'])

# 'admin_fee' is a constant so, it doesn't provide useful information for prediction.  should be removed. 

######## ploting for 'state_fee'
sns.boxplot(data=train_df,y='state_fee',x='compliance')

plt.scatter(range(len(train_df['state_fee'])),train_df['state_fee'])

# 'state_fee' is the same case as 'admin_fee'. should be removed. 


######## ploting for 'discount_amount'
plt.figure(figsize=(18,8))
plt.subplot(1,2,1)
sns.boxplot(data=train_df,y='discount_amount',x='compliance')
plt.title('Discount amount vs compliance')
plt.subplot(1,2,2)
sns.boxplot(data=train_df[train_df['discount_amount']!=0],y='discount_amount',x='compliance')
plt.title('Discount amount vs compliance (zoomed in)')


######## ploting for 'clean_up_cost'
sns.boxplot(data=train_df,y='clean_up_cost',x='compliance')

plt.scatter(range(len(train_df['clean_up_cost'])),train_df['clean_up_cost'])

# This variabvle only contains one value, should be removed.


######## ploting for 'judgment_amount'
plt.figure(figsize=(18,8))
plt.subplot(1,2,1)
sns.boxplot(data=train_df,y='judgment_amount',x='compliance')
plt.subplot(1,2,2)



 
#Dropping variables that don't provide information. 
train_df = train_df.drop(['admin_fee','state_fee','clean_up_cost'],axis=1)
test_df = test_df.drop(['admin_fee','state_fee','clean_up_cost'],axis=1)


################################################################################
    # Label encoding

#Label encoding categorical variables
le = LabelEncoder()
ohe = OneHotEncoder()

le.fit(train_df["agency_name"].append(test_df["agency_name"]))
train_df["agency_name"] = le.transform(train_df["agency_name"])
test_df["agency_name"] = le.transform(test_df["agency_name"])

le.fit(train_df["disposition"].append(test_df["disposition"]))
train_df["disposition"] = le.transform(train_df["disposition"])
test_df["disposition"] = le.transform(test_df["disposition"])


################################################################################
    # Data preparation
#to split the training data into the explanatory and target variables, and then split the resulting data into train and test sets.

#Splitting the training dataset into features and target variables
y = train_df["compliance"]
x = train_df.drop(["compliance"],axis=1)

#Train/Test Split
x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=0)


print (x_train.shape)
print (x_test.shape)
print (y_train.shape)
print (y_test.shape)



################################################################################
    #Modeling Simple Random Forest Regressor
    
#Defining lists for scoring metrics which will be used to store metrics for each modeling iteration
prec = [] 
rec = [] 
acc = [] 
auc = []
f1 = [] 
model_method = [] 
spec = []

def model_predict(model,x_train,x_test,y_train,y_test):  

  x_train = pd.DataFrame(x_train)
  x_train.columns = x.columns
  x_test = pd.DataFrame(x_test)
  x_test.columns = x.columns
  y_train = pd.DataFrame(y_train)
  #y_train.columns = 'compliance'
  y_test = pd.DataFrame(y_test)
  #y_test.columns = 'compliance'

  #Modeling and prediction
  model = model.fit(x_train,y_train)
  prediction = model.predict(x_test)

  #Calculating prediction probabilities
  lr_probs = model.predict_proba(x_test)
  lr_probs_pos = lr_probs[:,1]
  lr_probs_neg = lr_probs[:,0]

  #Calculating ROC AUC values
  lr_auc = roc_auc_score(y_test, prediction)
  lr_auc_pos = roc_auc_score(y_test, lr_probs_pos)
  lr_auc_neg = roc_auc_score(y_test, lr_probs_neg)

  print('Combined ROC AUC=%.3f' % (lr_auc))
  print('Severe class ROC AUC=%.3f' % (lr_auc_pos))
  print('Not severe class ROC AUC=%.3f' % (lr_auc_neg))

  lr_fpr, lr_tpr, thresh = roc_curve(y_test, prediction)
  lr_fpr_neg, lr_tpr_neg, thresh = roc_curve(y_test, lr_probs[:,0])
  lr_fpr_pos, lr_tpr_pos, thresh = roc_curve(y_test, lr_probs_pos)

  tn, fp, fn, tp = confusion_matrix(y_test.values, prediction).ravel()

  #Calculating scoring metrics
  precision = precision_score(y_test.values,prediction)
  accuracy = accuracy_score(y_test.values,prediction)
  roc_auc = lr_auc_pos
  recall = recall_score(y_test.values,prediction)
  f1 = f1_score(y_test.values,prediction)
  specificity = tn/(tn+fp)

  print ("ROC AUC: ",roc_auc)
  print ("Accuracy: ",accuracy)
  print ("Precision: ",precision)
  print ("Recall: ",recall)
  print ("F1 score: ",f1)
  print ("Specificity: ",specificity)

  #Plotting ROC AUC curve
  plt.figure(figsize=(9,7))
  # plot the roc curve for the model
  plt.plot(lr_fpr, lr_tpr, marker='.', label='Combined')
  plt.plot(lr_fpr_neg, lr_tpr_neg, marker='.', label='Not severe class')
  plt.plot(lr_fpr_pos, lr_tpr_pos, marker='.', label='Severe class')
  # axis labels
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive Rate')
  # show the legend
  plt.legend()
  # show the plot
  plt.show()

  # calculate precision-recall curve
  precision_curve, recall_curve, thresholds = precision_recall_curve(y_test, lr_probs[:,1])
  plt.figure(figsize=(9,7))
  plt.plot(recall_curve,precision_curve)
  plt.xlabel('Recall')
  plt.ylabel('Precision')
  plt.show()

  #Plotting confusion matrix
  plot_confusion_matrix(cm           = np.array([[ tp,  fn],
                                                [  fp, tn]]), 
                        normalize    = False,
                        target_names = ['Compliance', 'Non-compliance'],
                        title        = "Confusion Matrix")
  
  #model.n_features_
  #model.best_score_
  #model.booster_
  features = pd.DataFrame()
  features['Feature'] = x_train.columns
  features['Importance'] = model.feature_importances_
  print (features.sort_values('Importance',ascending=False))

  plt.figure(figsize=(12,8))
  sns.barplot(x='Feature',y='Importance',data=features.sort_values('Importance',ascending=False),alpha=0.7,palette='Blues_d')
  plt.xticks(rotation=45,size=18)
  plt.yticks(size=18)
  plt.xlabel('Feature',size=20)
  plt.ylabel('Score',size=20)
  plt.show()

  return precision,recall,accuracy,roc_auc,f1,specificity
  
  
  
  
  def plot_confusion_matrix(cm,
                          target_names,
                          title='Confusion matrix',
                          cmap=None,
                          normalize=True):
    """
    given a sklearn confusion matrix (cm), make a nice plot

    Arguments
    ---------
    cm:           confusion matrix from sklearn.metrics.confusion_matrix

    target_names: given classification classes such as [0, 1, 2]
                  the class names, for example: ['high', 'medium', 'low']

    title:        the text to display at the top of the matrix

    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm
                  see http://matplotlib.org/examples/color/colormaps_reference.html
                  plt.get_cmap('jet') or plt.cm.Blues

    normalize:    If False, plot the raw numbers
                  If True, plot the proportions

    Usage
    -----
    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by
                                                              # sklearn.metrics.confusion_matrix
                          normalize    = True,                # show proportions
                          target_names = y_labels_vals,       # list of names of the classes
                          title        = best_estimator_name) # title of graph

    Citiation
    ---------
    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html

    """
    import matplotlib.pyplot as plt
    import numpy as np
    import itertools

    accuracy = np.trace(cm) / float(np.sum(cm))
    misclass = 1 - accuracy

    if cmap is None:
        cmap = plt.get_cmap('Blues')

    plt.figure(figsize=(9, 7))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()

    if target_names is not None:
        tick_marks = np.arange(len(target_names))
        plt.xticks(tick_marks, target_names, rotation=45)
        plt.yticks(tick_marks, target_names)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]


    thresh = cm.max() / 1.5 if normalize else cm.max() / 2
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        if normalize:
            plt.text(j, i, "{:0.4f}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")
        else:
            plt.text(j, i, "{:,}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")


    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))
    plt.show()
    
  
  
  
#### Run baseline_randomforest model 

prec_score, rec_score, acc_score, auc_score, f_score, spec_score = model_predict(RandomForestClassifier(random_state=0),x_train,x_test,y_train,y_test)

model_method.append('baseline_randomforest')
prec.append(prec_score)
rec.append(rec_score)
acc.append(acc_score)
auc.append(auc_score)
f1.append(f_score)
spec.append(spec_score)
